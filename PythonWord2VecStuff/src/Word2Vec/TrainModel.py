#This program trains a word2vec model given an input file.
#The input file's name should be provided by the user as the first command line argument.  
#Possible values for this argument are 'entityreplaced', 'aliasreplaced', and 'original'.
#The word2vec file's name will be "word2vec.inputfilename"

import gensim, logging, numpy, os, sys

#SentenceIterator is a class that iterates over the training sentences.  
from SentenceIterator import SentenceIterator



#I don't know what this does, but some gensim tutorial told me to include this line.
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)




#Some parameters.  I set them to what I thought were reasonable defaults, or values we discussed.
hiddenlayersize = 100
windowsize = 5 #maximum distance between words looked at during training.
skipgramorbow = 1 #1 for skip gram.  0 for continuous bag of words.
wordoccurrencethreshold = 5 #If a word occurs at least this number of times in our dataset, include it in our vocabulary.
repeatrelevantdocumenttimes = 100 #We want our relevant documents to have more of an impact on the model than the wikipedia articles, so feed them into the training process multiple times.  This number should be greater than wordoccurrencethreshold, or some important vocabulary may be left out.



#Get the paths to a few different files we will use.  We assume that this program stays somewhere within the git project.
#This is the directory that this file is actually located in
programpath = os.path.dirname(os.path.realpath(__file__))

#This is the directory of the git project which includes both the Java project and this python project.
path_to_git_project = programpath
while os.path.basename(path_to_git_project) != "relation-bootstrap":
    path_to_git_project = os.path.abspath(os.path.join(path_to_git_project, ".."))
    
#This is the directory where all the different files produced by the Java (and this Python) project go.
produced_file_dir = os.path.abspath(os.path.join(path_to_git_project, "ProducedFiles"))
data_file_dir = os.path.abspath(os.path.join(path_to_git_project, "DataFiles"))

#This is the input file containing the training sentences, one per line.  
#It is generated by the java program gov.ornl.stucco.WriteTrainingInstances.
#Known versions of the file that can be used include 'entityreplaced', 'aliasreplaced', and 'original'.  One of these three things should
#be the only command line argument sent to this program.
#preprocessedfile = '/Users/p5r/stuccovm/preprocesseddata/aliassubstitutedentitynamesprocesseddocuments'
#entity_extracted_filename = 'aliasreplaced'
entity_extracted_filename = sys.argv[1]
preprocessedfile = os.path.join(produced_file_dir, 'EntityExtractedText', entity_extracted_filename + ".zip")
wikilemmatizeddir = os.path.join(data_file_dir, 'WikipediaLemmatized')

#These are the files this program can produce.  
#This file is the actual word2vec model.  But since we don't use the model for anything, I commented out everything having to do with writing it.
#word2vecmodelfile = '/Users/p5r/stuccovm/models/securityword2vecmodel'

#This file is where we write our word vectors, which we do actually use.
#wordvectorsfile = '/Users/p5r/stuccovm/models/wordvectors'
word_vectors_filename = "wordvectors." + sys.argv[1]
wordvectorsfile = os.path.join(produced_file_dir, 'Models', word_vectors_filename)



#Get the corpus sentences to train on.  sentences is actually an iterator, so we do not have to worry about trying to store it in its entirety in memory.
sentences = SentenceIterator(preprocessedfile, wikilemmatizeddir, repeatrelevantdocumenttimes) # a memory-friendly iterator



#Train the model.
model = gensim.models.Word2Vec(sentences, size=hiddenlayersize, workers=4, window=windowsize, sg=skipgramorbow, min_count=wordoccurrencethreshold, sorted_vocab=1)    



#Write the word vectors to a file.  
#This is the only result we actually 
#The gensim api does not specify if model[k] is k's in or out vector.
os.makedirs(os.path.dirname(wordvectorsfile))
wordvectorsfilestream = open(wordvectorsfile, 'w+')
for (k, v) in model.vocab.iteritems():
    #print(k + "\t" + model[k])
    wordvectorsfilestream.write(k)
    numpyarray = model[k]
    for value in numpy.nditer(numpyarray):
        wordvectorsfilestream.write(" " + str(value))
    wordvectorsfilestream.write("\n")
wordvectorsfilestream.close()
    


#Save the word2vec model too.
#Only, since we never actually use the model, I just commented it out, and we will throw it away.
#model.save(word2vecmodelfile)




